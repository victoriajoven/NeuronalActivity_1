{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fcfe637",
   "metadata": {},
   "source": [
    "## Techniques to improve BP-F\n",
    "We improved the model by adding a combination of parameters L2 + Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e6a2f2",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Added imports with l2 and EarlyStopping for improbe the BP-F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c45dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff651a",
   "metadata": {},
   "source": [
    "## Improve the model in this definition\n",
    "- using kernel_regularizer=l2(0.001) for for more accurate predictions with a smaller margin of error.\n",
    "in the definition of the layers\n",
    "- using Dropout turns off 20% of the neurons randomly in each batch because it forces learning to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bfef359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "X_train: (800, 23)\n",
      "X_test : (200, 23)\n",
      "y_train: (800,)\n",
      "y_test : (200,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\Documents\\masterciberseguridad\\secondyear\\Neuronal\\Code\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_path = Path(os.path.abspath(\"03_model_comparison.ipynb\")).parent.parent\n",
    "X_train = np.load(base_path / \"data/processed/X_train_preprocessed.npy\")\n",
    "X_test  = np.load(base_path / \"data/processed/X_test_preprocessed.npy\")\n",
    "\n",
    "y_train = np.load(base_path / \"data/learn/y_train.npy\")\n",
    "y_test  = np.load(base_path / \"data/check/y_test.npy\")\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test :\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test :\", y_test.shape)\n",
    "\n",
    "\n",
    "model_bp_reg = Sequential([\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.001), input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffced68",
   "metadata": {},
   "source": [
    "## Add previous compilation\n",
    "It's necessary for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9c9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bp_reg.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a950ebf7",
   "metadata": {},
   "source": [
    "## EarlyStopping\n",
    "Earlstops training when the model stops improvingyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e5dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2b8d3",
   "metadata": {},
   "source": [
    "## Model training\n",
    "Model training is done using the fit() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8bd7473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 15501.4160 - mae: 87.2326 - val_loss: 16362.3516 - val_mae: 88.8106\n",
      "Epoch 2/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15243.2109 - mae: 86.6449 - val_loss: 15987.6582 - val_mae: 88.0158\n",
      "Epoch 3/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14769.1855 - mae: 85.5079 - val_loss: 15254.9268 - val_mae: 86.2750\n",
      "Epoch 4/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13779.6143 - mae: 82.8085 - val_loss: 13895.6436 - val_mae: 82.6304\n",
      "Epoch 5/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12082.0430 - mae: 77.6087 - val_loss: 11626.8691 - val_mae: 75.7263\n",
      "Epoch 6/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9546.9336 - mae: 68.8949 - val_loss: 8629.3574 - val_mae: 65.0390\n",
      "Epoch 7/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6494.3564 - mae: 56.4802 - val_loss: 5192.4419 - val_mae: 50.2919\n",
      "Epoch 8/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3579.4800 - mae: 41.6376 - val_loss: 2498.2383 - val_mae: 36.3477\n",
      "Epoch 9/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1653.3949 - mae: 29.6045 - val_loss: 1205.8978 - val_mae: 27.2972\n",
      "Epoch 10/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1087.2061 - mae: 25.3146 - val_loss: 802.7787 - val_mae: 23.3699\n",
      "Epoch 11/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 861.6282 - mae: 22.1626 - val_loss: 648.6151 - val_mae: 20.7944\n",
      "Epoch 12/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 797.3141 - mae: 20.8093 - val_loss: 547.5655 - val_mae: 18.3693\n",
      "Epoch 13/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 711.7177 - mae: 18.7968 - val_loss: 470.7997 - val_mae: 16.5224\n",
      "Epoch 14/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 637.5826 - mae: 17.2583 - val_loss: 405.9341 - val_mae: 15.0085\n",
      "Epoch 15/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 616.8796 - mae: 16.3688 - val_loss: 358.3170 - val_mae: 13.6316\n",
      "Epoch 16/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 589.0927 - mae: 15.4558 - val_loss: 316.3098 - val_mae: 12.4502\n",
      "Epoch 17/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 557.1223 - mae: 14.9582 - val_loss: 292.4342 - val_mae: 11.5964\n",
      "Epoch 18/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 511.5045 - mae: 14.0794 - val_loss: 267.4980 - val_mae: 10.9459\n",
      "Epoch 19/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 547.5795 - mae: 14.2805 - val_loss: 243.6316 - val_mae: 10.3748\n",
      "Epoch 20/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 410.8346 - mae: 12.6662 - val_loss: 227.0419 - val_mae: 10.0140\n",
      "Epoch 21/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 453.6938 - mae: 13.1743 - val_loss: 215.1787 - val_mae: 9.7586\n",
      "Epoch 22/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 469.8667 - mae: 13.5083 - val_loss: 198.9839 - val_mae: 9.4102\n",
      "Epoch 23/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 424.1762 - mae: 12.8269 - val_loss: 182.9146 - val_mae: 9.1000\n",
      "Epoch 24/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 465.9494 - mae: 13.3641 - val_loss: 174.9429 - val_mae: 8.8936\n",
      "Epoch 25/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 395.8748 - mae: 12.4920 - val_loss: 164.8636 - val_mae: 8.6545\n",
      "Epoch 26/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 392.8210 - mae: 12.4877 - val_loss: 162.4179 - val_mae: 8.5401\n",
      "Epoch 27/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 401.3215 - mae: 12.6339 - val_loss: 169.4465 - val_mae: 8.5976\n",
      "Epoch 28/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 426.6127 - mae: 12.5287 - val_loss: 164.6680 - val_mae: 8.4747\n",
      "Epoch 29/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 382.3577 - mae: 11.8930 - val_loss: 143.2937 - val_mae: 8.0221\n",
      "Epoch 30/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 371.4006 - mae: 11.5390 - val_loss: 144.9326 - val_mae: 8.0087\n",
      "Epoch 31/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 412.2172 - mae: 12.3359 - val_loss: 131.1655 - val_mae: 7.6350\n",
      "Epoch 32/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 410.1744 - mae: 12.0167 - val_loss: 130.4304 - val_mae: 7.5739\n",
      "Epoch 33/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 394.2943 - mae: 12.2356 - val_loss: 126.4938 - val_mae: 7.4568\n",
      "Epoch 34/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 443.5775 - mae: 12.5438 - val_loss: 119.4605 - val_mae: 7.2108\n",
      "Epoch 35/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 402.5198 - mae: 12.3346 - val_loss: 123.4218 - val_mae: 7.3267\n",
      "Epoch 36/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 370.8761 - mae: 11.4933 - val_loss: 136.8398 - val_mae: 7.5929\n",
      "Epoch 37/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 335.3476 - mae: 11.0035 - val_loss: 127.3119 - val_mae: 7.3523\n",
      "Epoch 38/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 403.3295 - mae: 11.9993 - val_loss: 134.2499 - val_mae: 7.4565\n",
      "Epoch 39/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 325.4986 - mae: 10.8797 - val_loss: 129.7187 - val_mae: 7.3661\n",
      "Epoch 40/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 357.9600 - mae: 11.4253 - val_loss: 125.9004 - val_mae: 7.2792\n",
      "Epoch 41/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 317.8634 - mae: 10.8249 - val_loss: 120.2441 - val_mae: 7.1459\n",
      "Epoch 42/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 357.7607 - mae: 11.3886 - val_loss: 122.9349 - val_mae: 7.1783\n",
      "Epoch 43/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 383.4873 - mae: 12.1497 - val_loss: 134.9990 - val_mae: 7.3951\n",
      "Epoch 44/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 352.8385 - mae: 11.5049 - val_loss: 119.5244 - val_mae: 7.0798\n",
      "Epoch 45/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 345.5721 - mae: 11.3752 - val_loss: 117.4824 - val_mae: 6.9462\n",
      "Epoch 46/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 344.9177 - mae: 11.2768 - val_loss: 112.8685 - val_mae: 6.7712\n",
      "Epoch 47/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 331.5937 - mae: 10.9203 - val_loss: 127.4994 - val_mae: 7.1190\n",
      "Epoch 48/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 339.6485 - mae: 11.0933 - val_loss: 117.4968 - val_mae: 6.9246\n",
      "Epoch 49/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 415.9950 - mae: 12.1072 - val_loss: 120.6373 - val_mae: 6.9956\n",
      "Epoch 50/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 407.6383 - mae: 12.1133 - val_loss: 114.1324 - val_mae: 6.8850\n",
      "Epoch 51/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 326.2519 - mae: 10.8199 - val_loss: 137.8626 - val_mae: 7.2912\n",
      "Epoch 52/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 352.8543 - mae: 11.3681 - val_loss: 123.2734 - val_mae: 6.9845\n",
      "Epoch 53/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 438.0240 - mae: 12.3928 - val_loss: 114.9616 - val_mae: 6.7585\n",
      "Epoch 54/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 437.8573 - mae: 12.3675 - val_loss: 111.3235 - val_mae: 6.6633\n",
      "Epoch 55/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 367.0395 - mae: 11.3015 - val_loss: 115.7464 - val_mae: 6.7359\n",
      "Epoch 56/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 334.7738 - mae: 10.9511 - val_loss: 108.6738 - val_mae: 6.6065\n",
      "Epoch 57/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 370.9683 - mae: 11.6703 - val_loss: 151.2742 - val_mae: 7.5192\n",
      "Epoch 58/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 372.8020 - mae: 11.5208 - val_loss: 118.2283 - val_mae: 6.8288\n",
      "Epoch 59/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 330.0151 - mae: 11.0060 - val_loss: 106.6042 - val_mae: 6.6258\n",
      "Epoch 60/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 342.6104 - mae: 11.1876 - val_loss: 124.1698 - val_mae: 6.9099\n",
      "Epoch 61/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 359.9794 - mae: 11.0647 - val_loss: 116.2511 - val_mae: 6.7237\n",
      "Epoch 62/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 322.1295 - mae: 10.8034 - val_loss: 106.1129 - val_mae: 6.4890\n",
      "Epoch 63/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 308.9425 - mae: 10.3957 - val_loss: 110.4793 - val_mae: 6.5149\n",
      "Epoch 64/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 308.5948 - mae: 10.6104 - val_loss: 99.9609 - val_mae: 6.3067\n",
      "Epoch 65/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 343.7372 - mae: 10.5634 - val_loss: 105.4517 - val_mae: 6.3894\n",
      "Epoch 66/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 337.6125 - mae: 10.9980 - val_loss: 102.9427 - val_mae: 6.3756\n",
      "Epoch 67/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 330.0599 - mae: 11.1570 - val_loss: 98.4601 - val_mae: 6.2524\n",
      "Epoch 68/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 336.2562 - mae: 10.8449 - val_loss: 106.8456 - val_mae: 6.4143\n",
      "Epoch 69/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 306.7102 - mae: 10.6491 - val_loss: 91.8434 - val_mae: 6.0873\n",
      "Epoch 70/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 307.7565 - mae: 10.6332 - val_loss: 108.8362 - val_mae: 6.4421\n",
      "Epoch 71/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 303.4246 - mae: 10.4313 - val_loss: 96.9927 - val_mae: 6.2109\n",
      "Epoch 72/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 385.0695 - mae: 11.4921 - val_loss: 92.2529 - val_mae: 5.9982\n",
      "Epoch 73/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 309.7774 - mae: 10.5824 - val_loss: 112.0623 - val_mae: 6.4628\n",
      "Epoch 74/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 339.8632 - mae: 11.0870 - val_loss: 111.6156 - val_mae: 6.5170\n",
      "Epoch 75/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 347.0454 - mae: 11.1795 - val_loss: 96.9725 - val_mae: 6.1910\n",
      "Epoch 76/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 314.4914 - mae: 10.7799 - val_loss: 115.3287 - val_mae: 6.5399\n",
      "Epoch 77/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 347.2570 - mae: 10.9568 - val_loss: 95.9697 - val_mae: 6.0946\n",
      "Epoch 78/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 335.6472 - mae: 11.0210 - val_loss: 90.6689 - val_mae: 5.9553\n",
      "Epoch 79/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 369.8708 - mae: 11.1770 - val_loss: 93.4663 - val_mae: 6.0133\n",
      "Epoch 80/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 356.4144 - mae: 11.3383 - val_loss: 90.5627 - val_mae: 5.9120\n",
      "Epoch 81/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 303.6318 - mae: 10.1926 - val_loss: 102.5312 - val_mae: 6.1769\n",
      "Epoch 82/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 290.4854 - mae: 10.1531 - val_loss: 125.0250 - val_mae: 6.6863\n",
      "Epoch 83/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 301.0652 - mae: 10.4105 - val_loss: 88.2231 - val_mae: 5.9035\n",
      "Epoch 84/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 349.2872 - mae: 11.1233 - val_loss: 104.7480 - val_mae: 6.2085\n",
      "Epoch 85/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 323.3513 - mae: 10.6987 - val_loss: 92.8936 - val_mae: 5.9696\n",
      "Epoch 86/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 365.6017 - mae: 11.2282 - val_loss: 92.9740 - val_mae: 5.9277\n",
      "Epoch 87/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 363.8571 - mae: 11.2977 - val_loss: 107.9190 - val_mae: 6.3492\n",
      "Epoch 88/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 294.6727 - mae: 10.1364 - val_loss: 97.6295 - val_mae: 6.0489\n",
      "Epoch 89/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 347.3620 - mae: 11.0768 - val_loss: 101.9304 - val_mae: 6.1835\n",
      "Epoch 90/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 318.6921 - mae: 10.5197 - val_loss: 89.0879 - val_mae: 5.7964\n",
      "Epoch 91/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 302.3410 - mae: 10.5791 - val_loss: 99.4570 - val_mae: 6.0872\n",
      "Epoch 92/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 333.6307 - mae: 10.7290 - val_loss: 100.3812 - val_mae: 6.1145\n",
      "Epoch 93/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 340.3260 - mae: 10.7302 - val_loss: 103.7553 - val_mae: 6.2837\n",
      "Epoch 94/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 324.6537 - mae: 10.5226 - val_loss: 91.7334 - val_mae: 5.9539\n",
      "Epoch 95/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 309.7833 - mae: 10.4474 - val_loss: 100.6055 - val_mae: 6.1512\n",
      "Epoch 96/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 314.9000 - mae: 10.6610 - val_loss: 102.3485 - val_mae: 6.1977\n",
      "Epoch 97/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 317.6893 - mae: 10.4726 - val_loss: 106.2626 - val_mae: 6.3258\n",
      "Epoch 98/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 329.7523 - mae: 10.5147 - val_loss: 95.7841 - val_mae: 6.1302\n",
      "Epoch 99/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 330.2052 - mae: 10.9283 - val_loss: 85.2163 - val_mae: 5.7540\n",
      "Epoch 100/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 316.2204 - mae: 10.4317 - val_loss: 97.9728 - val_mae: 6.0794\n",
      "Epoch 101/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 352.1888 - mae: 10.5429 - val_loss: 93.5338 - val_mae: 5.9648\n",
      "Epoch 102/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 397.6387 - mae: 11.4850 - val_loss: 97.7264 - val_mae: 6.1657\n",
      "Epoch 103/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 314.8503 - mae: 10.3972 - val_loss: 92.4093 - val_mae: 6.0215\n",
      "Epoch 104/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 337.9088 - mae: 10.8909 - val_loss: 89.1873 - val_mae: 5.9462\n",
      "Epoch 105/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 318.3883 - mae: 10.7617 - val_loss: 91.7215 - val_mae: 5.9527\n",
      "Epoch 106/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 361.4943 - mae: 11.2706 - val_loss: 90.6899 - val_mae: 5.9131\n",
      "Epoch 107/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 280.5695 - mae: 10.0490 - val_loss: 84.3198 - val_mae: 5.7386\n",
      "Epoch 108/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 329.0906 - mae: 10.8182 - val_loss: 94.8287 - val_mae: 6.0066\n",
      "Epoch 109/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 324.7577 - mae: 10.5871 - val_loss: 85.4335 - val_mae: 5.8106\n",
      "Epoch 110/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 346.7938 - mae: 10.9384 - val_loss: 86.4931 - val_mae: 5.6916\n",
      "Epoch 111/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 354.6422 - mae: 10.9970 - val_loss: 78.0466 - val_mae: 5.5557\n",
      "Epoch 112/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 305.7582 - mae: 10.5706 - val_loss: 81.4478 - val_mae: 5.6233\n",
      "Epoch 113/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 344.4912 - mae: 10.8417 - val_loss: 99.0189 - val_mae: 6.0718\n",
      "Epoch 114/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 342.3308 - mae: 10.8114 - val_loss: 105.7068 - val_mae: 6.2714\n",
      "Epoch 115/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 303.3151 - mae: 10.3266 - val_loss: 91.0018 - val_mae: 6.0031\n",
      "Epoch 116/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 346.5984 - mae: 10.7049 - val_loss: 90.0410 - val_mae: 5.9719\n",
      "Epoch 117/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 334.1178 - mae: 10.5791 - val_loss: 85.3425 - val_mae: 5.8369\n",
      "Epoch 118/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 350.8874 - mae: 11.0940 - val_loss: 73.7917 - val_mae: 5.4071\n",
      "Epoch 119/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 309.4501 - mae: 10.2752 - val_loss: 97.2183 - val_mae: 6.0589\n",
      "Epoch 120/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 323.7797 - mae: 10.6877 - val_loss: 70.2249 - val_mae: 5.2888\n",
      "Epoch 121/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 317.9543 - mae: 10.6447 - val_loss: 92.8860 - val_mae: 5.9436\n",
      "Epoch 122/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 309.4851 - mae: 10.2614 - val_loss: 87.9150 - val_mae: 5.8670\n",
      "Epoch 123/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 328.9828 - mae: 10.7832 - val_loss: 93.9891 - val_mae: 6.0933\n",
      "Epoch 124/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 341.5923 - mae: 10.8153 - val_loss: 84.1828 - val_mae: 5.8577\n",
      "Epoch 125/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 331.0817 - mae: 10.4594 - val_loss: 84.2892 - val_mae: 5.8400\n",
      "Epoch 126/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 333.1116 - mae: 10.9285 - val_loss: 82.1289 - val_mae: 5.7214\n",
      "Epoch 127/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 330.7530 - mae: 10.5639 - val_loss: 105.9541 - val_mae: 6.3113\n",
      "Epoch 128/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 307.8764 - mae: 10.3564 - val_loss: 78.8445 - val_mae: 5.6141\n",
      "Epoch 129/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 302.5632 - mae: 10.2907 - val_loss: 74.1089 - val_mae: 5.4852\n",
      "Epoch 130/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 319.8011 - mae: 10.8218 - val_loss: 88.3351 - val_mae: 5.8324\n",
      "Epoch 131/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 320.8946 - mae: 10.4785 - val_loss: 91.1936 - val_mae: 5.9641\n",
      "Epoch 132/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 304.7791 - mae: 10.6413 - val_loss: 92.9521 - val_mae: 6.0061\n",
      "Epoch 133/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 309.3197 - mae: 10.4966 - val_loss: 73.7946 - val_mae: 5.5051\n",
      "Epoch 134/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 333.5610 - mae: 10.4818 - val_loss: 81.3354 - val_mae: 5.6797\n",
      "Epoch 135/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 381.5455 - mae: 11.2870 - val_loss: 78.2947 - val_mae: 5.5693\n",
      "Epoch 136/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 297.0165 - mae: 9.8553 - val_loss: 104.9980 - val_mae: 6.3391\n",
      "Epoch 137/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 318.0499 - mae: 10.3727 - val_loss: 82.7307 - val_mae: 5.8223\n",
      "Epoch 138/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 321.4970 - mae: 10.5912 - val_loss: 84.6612 - val_mae: 5.8122\n",
      "Epoch 139/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 328.6114 - mae: 10.4347 - val_loss: 91.8848 - val_mae: 5.9961\n",
      "Epoch 140/300\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 292.2383 - mae: 9.7561 - val_loss: 88.6695 - val_mae: 5.8860\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_bp_reg = model_bp_reg.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=300,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3826234",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ad685ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 100.4106 - mae: 6.3796\n",
      "\n",
      "---- BP-F Regularized (L2 + Dropout) ----\n",
      "MSE: 100.41056823730469\n",
      "MAE: 6.379579544067383\n"
     ]
    }
   ],
   "source": [
    "mse_bp_reg, mae_bp_reg = model_bp_reg.evaluate(X_test, y_test)\n",
    "print(\"\\n---- BP-F Regularized (L2 + Dropout) ----\")\n",
    "print(\"MSE:\", mse_bp_reg)\n",
    "print(\"MAE:\", mae_bp_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411db00",
   "metadata": {},
   "source": [
    "## Manual MAPE\n",
    "Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27ab178e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MAPE: 103.39037157203042\n"
     ]
    }
   ],
   "source": [
    "pred_bp_reg = model_bp_reg.predict(X_test)\n",
    "# Epsilon added to avoid divide by 0\n",
    "mask = y_test != 0\n",
    "mape_bp_reg = np.mean(np.abs((y_test[mask] - pred_bp_reg[mask]) / y_test[mask])) * 100\n",
    "\n",
    "print(\"MAPE:\", mape_bp_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f30d99",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- Regularized is more stable because it prevents overfitting, produces a more robust model, improves the average errors (MAE), and reduces the model's variance.\n",
    "- With a low MAE (6.38) indicating good average accuracy in CO2 predictions.\n",
    "- In this exercise, the problem of having to avoid division by 0 or epsilon for the MAPE values has arisen, because otherwise it would give an inconsistent value.\n",
    "- Although the model shows relatively low MAE and MSE, the MAPE ≈ 103% suggests a high percentage error. This behavior is expected due to the presence of very small values in the output variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
